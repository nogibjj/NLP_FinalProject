"""Demonstrate Naive Bayes classification."""
import random
from typing import List, Mapping, Optional, Sequence
import nltk
import numpy as np
from numpy.typing import NDArray
import math
import pandas as pd

FloatArray = NDArray[np.float64]


# define model
class UnigramModel:
    """The unigram language model."""

    def __init__(self, size: int) -> None:
        """Initialize."""
        self.size = size
        self.p: Optional[FloatArray] = None

    def train(self, encodings: List[FloatArray]) -> "UnigramModel":
        """Train the model on data."""
        counts = np.ones((self.size, 1))
        for encoding in encodings:
            counts += encoding
        self.p = counts / counts.sum()
        return self

    def apply(self, encodings: List[FloatArray]) -> float:
        """Compute the log probability of a document."""
        if self.p is None:
            raise ValueError("This model is untrained")
        return (
            np.hstack(encodings).sum(axis=1, keepdims=True).T @ np.log(self.p)
        ).item()


completeSpam = pd.read_csv(
    "/Users/leilei/Downloads/MIDS/703 NLP/Final Project/Final Project/NLP_finalProject/Alicia_workspace/completeSpamAssassin.csv"
)
# 删除空行
df = pd.DataFrame(completeSpam)


def process_email_body_simple(body):
    """
    Process the email body by splitting it into lines, and then splitting each line into words.
    Handles cases where the body is not a string. No quoting of words.
    """
    if not isinstance(body, str):
        return []

    # Split the email body into lines
    lines = body.split("\n")

    # Split each line into words and store them in a list
    processed_lines = [line.split() for line in lines if line.strip() != ""]

    return processed_lines


# Splitting the dataframe into two based on the label
spam_df = df[df["Label"] == 1]
non_spam_df = df[df["Label"] == 0]

# Applying the simplified processing function to each group
spam_emails = spam_df["Body"].apply(process_email_body_simple)
non_spam_emails = non_spam_df["Body"].apply(process_email_body_simple)

# Compile the processed emails into separate lists for spam and non-spam
spam_list = [line for email in spam_emails for line in email]
non_spam_list = [line for email in non_spam_emails for line in email]

# Display the first few elements of the compiled simple list to verify the structure
sample_spam = spam_list[:300]
sample_nonspam = non_spam_list[:500]

vocabulary = sorted(
    set(token for sentence in sample_spam + sample_nonspam for token in sentence)
) + [None]
vocabulary_map = {token: idx for idx, token in enumerate(vocabulary)}


def onehot(
    vocabulary_map: Mapping[Optional[str], int], token: Optional[str]
) -> FloatArray:
    """Generate the one-hot encoding for the provided token in the provided vocabulary."""
    embedding = np.zeros((len(vocabulary_map), 1))
    idx = vocabulary_map.get(token, len(vocabulary_map) - 1)
    embedding[idx, 0] = 1
    return embedding


def encode_document(tokens: Sequence[Optional[str]]) -> List[FloatArray]:
    """Apply one-hot encoding to each document."""
    encodings = [onehot(vocabulary_map, token) for token in tokens]
    return encodings


# assemble training and testing data
h0_observations = [(encode_document(sentence), 0) for sentence in sample_spam]
h1_observations = [(encode_document(sentence), 1) for sentence in sample_nonspam]
all_data = h0_observations + h1_observations
random.shuffle(all_data)
test_percent = 10
break_idx = round(test_percent / 100 * len(all_data))
training_data = all_data[break_idx:]
testing_data = all_data[:break_idx]

# train Naive Bayes
h0_documents = [observation[0] for observation in training_data if observation[1] == 0]
h1_documents = [observation[0] for observation in training_data if observation[1] == 1]
h0_language_model = UnigramModel(len(vocabulary_map))
h0_language_model.train([token for document in h0_documents for token in document])
h1_language_model = UnigramModel(len(vocabulary_map))
h1_language_model.train([token for document in h1_documents for token in document])
ph0 = len(h0_documents) / len(training_data)
ph1 = len(h1_documents) / len(training_data)

num_correct = 0
for document, label in testing_data:
    # apply model for each class
    h0_logp_unnormalized = h0_language_model.apply(document) + np.log(ph0)
    h1_logp_unnormalized = h1_language_model.apply(document) + np.log(ph1)

    # normalize
    logp_data = np.logaddexp(h0_logp_unnormalized, h1_logp_unnormalized)
    h0_logp = h0_logp_unnormalized - logp_data
    h1_logp = h1_logp_unnormalized - logp_data

    # make guess
    pc0 = np.exp(h0_logp)
    pc1 = np.exp(h1_logp)
    guess = 1 if pc1 > pc0 else 0
    print(pc0, pc1, guess, label)

    if guess == label:
        num_correct += 1

print(num_correct / len(testing_data))
